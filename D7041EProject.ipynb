{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "## D7041E project by Sebastian Lindgren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.datasets import mnist\n",
    "from collections import OrderedDict\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from ipywidgets import IntProgress\n",
    "from IPython.display import display\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available. GPU is available for use.\n"
     ]
    }
   ],
   "source": [
    "# Test if cuda is working\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available. GPU is available for use.\")\n",
    "else:\n",
    "    print(\"CUDA is not available. Using CPU instead.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 20000 training samples and 5000 test samples\n"
     ]
    }
   ],
   "source": [
    "# Load minst dataset\n",
    "(Xtr, Ltr), (X_test, L_test)=mnist.load_data()\n",
    "\n",
    "# Choose training and test data\n",
    "num_sample_training=20000\n",
    "num_sample_testing=5000\n",
    "X_training=Xtr[:num_sample_training,:,:]\n",
    "L_training=Ltr[:num_sample_training]\n",
    "X_test=X_test[:num_sample_testing,:,:]\n",
    "L_test=L_test[:num_sample_testing]\n",
    "\n",
    "img_shape = Xtr.shape[1]*Xtr.shape[2]\n",
    "norm_value = 255\n",
    "\n",
    "# Flatten to 1D array and normalize data\n",
    "X_training=X_training.reshape(num_sample_training, img_shape) / norm_value\n",
    "X_test=X_test.reshape(num_sample_testing, img_shape) / norm_value\n",
    "\n",
    "# USe GPU instead of CPU to process faster\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Convert to tensor and move GPU if it's available\n",
    "X_training = torch.tensor(X_training, dtype=torch.float32).to(device)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "L_training = torch.tensor(L_training).long().to(device)  \n",
    "L_test = torch.tensor(L_test).long().to(device)\n",
    "\n",
    "train_dataset = TensorDataset(X_training, L_training)\n",
    "test_dataset = TensorDataset(X_test, L_test)\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "# Shuffle the trainingdata\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"Using {X_training.shape[0]} training samples and {X_test.shape[0]} test samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, hidden_layers, start_neurons) -> None:\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        layers_list = [('input', nn.Linear(784, start_neurons)), ('input_act', nn.ReLU())]\n",
    "\n",
    "        # Add the specified number of hidden layers\n",
    "        neurons = start_neurons\n",
    "        for i in range(hidden_layers):\n",
    "            out_neurons = neurons//2 if neurons >= 20 else neurons\n",
    "            layers_list.append((f'dense{i+1}', nn.Linear(neurons, out_neurons))) \n",
    "            layers_list.append((f'act{i+1}', nn.ReLU())) \n",
    "            neurons = out_neurons\n",
    "      \n",
    "        layers_list.append(('output', nn.Linear(neurons, 10)))  \n",
    "        \n",
    "        self.model_layers = OrderedDict(layers_list)\n",
    "\n",
    "        self.model = nn.Sequential(self.model_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "    def train_model(self, train_loader, num_epochs, optimizer , loss_fn=nn.CrossEntropyLoss()):\n",
    "        self.train()  # Set the model to training mode\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            running_loss = 0\n",
    "            for images, labels in train_loader:\n",
    "                # Move data and labels to the same device as the model\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "    \n",
    "                # Zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = self(images)\n",
    "                loss = loss_fn(outputs, labels)\n",
    "                \n",
    "                \n",
    "                # Backward and optimize\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "            \n",
    "            # Print statistics\n",
    "            #print(f\"Epoch {epoch+1}/{num_epochs}.. \"\n",
    "                  #f\"Train loss: {running_loss/len(train_loader):.3f}\")\n",
    "\n",
    "    def test_model(self, test_loader, loss_fn=nn.CrossEntropyLoss()):\n",
    "        self.eval()  \n",
    "\n",
    "        test_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():  # No need to track gradients while testing, this makes it faster\n",
    "            for images, labels in test_loader:\n",
    "               \n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = self(images)\n",
    "                test_loss += loss_fn(outputs, labels).item()\n",
    "                \n",
    "\n",
    "                # Get the predicted class from the maximum value in the output-list of class scores\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        \n",
    "        avg_loss = test_loss / len(test_loader)\n",
    "        accuracy = correct / total\n",
    "        #print(f'Test Loss: {avg_loss:.4f}, Test Accuracy: {accuracy:.4f}')\n",
    "        return accuracy\n",
    "\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train and test model\n",
    "# starting_neurons_list = [32, 64, 256, 512, 1024]\n",
    "# hidden_layers_list = [1, 2, 3, 4, 5]\n",
    "# epochs_list = [ 10, 20, 50, 70]\n",
    "# hidden_layers = 1\n",
    "# starting_neurons = 256\n",
    "# epochs = 10\n",
    "\n",
    "# model = MLP(hidden_layers=hidden_layers, start_neurons=starting_neurons).to(device)\n",
    "# print(f\"This is the current model: {model}\")\n",
    "# loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# model.train_model(train_loader=train_loader, num_epochs=epochs, loss_fn=loss_fn, optimizer=optimizer)\n",
    "\n",
    "# accuracy = model.test_model(test_loader=test_loader, loss_fn=loss_fn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f56da028ba594e0b80f5841c02416fe0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, max=64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best accuracy: 0.9724. It was achieved with the following hyperparmeters: \n",
      " 70 epochs. \n",
      " 512 starting neurons. \n",
      " 2 layers. \n",
      " Adam as optimizer. \n",
      "Epochs: 70, Layers: 2, Neurons: 512, Accuracy: 0.9724, Optimizer: Adam \n",
      "Epochs: 70, Layers: 3, Neurons: 512, Accuracy: 0.9694, Optimizer: Adam \n",
      "Epochs: 40, Layers: 2, Neurons: 512, Accuracy: 0.969, Optimizer: Adam \n",
      "Epochs: 40, Layers: 1, Neurons: 256, Accuracy: 0.9682, Optimizer: Adam \n",
      "Epochs: 40, Layers: 2, Neurons: 256, Accuracy: 0.9678, Optimizer: Adam \n",
      "Epochs: 70, Layers: 1, Neurons: 256, Accuracy: 0.9676, Optimizer: RMSprop \n",
      "Epochs: 70, Layers: 4, Neurons: 512, Accuracy: 0.9674, Optimizer: RMSprop \n",
      "Epochs: 70, Layers: 2, Neurons: 256, Accuracy: 0.9672, Optimizer: Adam \n",
      "Epochs: 70, Layers: 1, Neurons: 512, Accuracy: 0.9672, Optimizer: Adam \n",
      "Epochs: 70, Layers: 2, Neurons: 512, Accuracy: 0.9672, Optimizer: RMSprop \n",
      "Epochs: 40, Layers: 3, Neurons: 256, Accuracy: 0.9668, Optimizer: RMSprop \n",
      "Epochs: 70, Layers: 1, Neurons: 256, Accuracy: 0.9666, Optimizer: Adam \n",
      "Epochs: 5, Layers: 1, Neurons: 512, Accuracy: 0.966, Optimizer: RMSprop \n",
      "Epochs: 70, Layers: 1, Neurons: 512, Accuracy: 0.966, Optimizer: SGD \n",
      "Epochs: 40, Layers: 2, Neurons: 256, Accuracy: 0.9656, Optimizer: RMSprop \n",
      "Epochs: 40, Layers: 2, Neurons: 512, Accuracy: 0.9656, Optimizer: RMSprop \n",
      "Epochs: 10, Layers: 1, Neurons: 256, Accuracy: 0.9654, Optimizer: RMSprop \n",
      "Epochs: 40, Layers: 3, Neurons: 512, Accuracy: 0.965, Optimizer: Adam \n",
      "Epochs: 40, Layers: 4, Neurons: 512, Accuracy: 0.965, Optimizer: Adam \n",
      "Epochs: 40, Layers: 4, Neurons: 512, Accuracy: 0.965, Optimizer: RMSprop \n",
      "Epochs: 40, Layers: 2, Neurons: 512, Accuracy: 0.9648, Optimizer: SGD \n",
      "Epochs: 70, Layers: 3, Neurons: 256, Accuracy: 0.9648, Optimizer: RMSprop \n",
      "Epochs: 10, Layers: 2, Neurons: 256, Accuracy: 0.9646, Optimizer: RMSprop \n",
      "Epochs: 70, Layers: 1, Neurons: 256, Accuracy: 0.9646, Optimizer: SGD \n",
      "Epochs: 70, Layers: 3, Neurons: 256, Accuracy: 0.9646, Optimizer: Adam \n",
      "Epochs: 10, Layers: 3, Neurons: 256, Accuracy: 0.9642, Optimizer: RMSprop \n",
      "Epochs: 40, Layers: 1, Neurons: 512, Accuracy: 0.964, Optimizer: SGD \n",
      "Epochs: 70, Layers: 4, Neurons: 256, Accuracy: 0.964, Optimizer: RMSprop \n",
      "Epochs: 70, Layers: 1, Neurons: 512, Accuracy: 0.964, Optimizer: RMSprop \n",
      "Epochs: 70, Layers: 3, Neurons: 512, Accuracy: 0.9634, Optimizer: RMSprop \n",
      "Epochs: 40, Layers: 1, Neurons: 512, Accuracy: 0.963, Optimizer: Adam \n",
      "Epochs: 40, Layers: 3, Neurons: 512, Accuracy: 0.963, Optimizer: SGD \n",
      "Epochs: 70, Layers: 4, Neurons: 256, Accuracy: 0.963, Optimizer: Adam \n",
      "Epochs: 40, Layers: 2, Neurons: 256, Accuracy: 0.9626, Optimizer: SGD \n",
      "Epochs: 40, Layers: 1, Neurons: 512, Accuracy: 0.9624, Optimizer: RMSprop \n",
      "Epochs: 70, Layers: 4, Neurons: 512, Accuracy: 0.9624, Optimizer: Adam \n",
      "Epochs: 10, Layers: 1, Neurons: 512, Accuracy: 0.9622, Optimizer: Adam \n",
      "Epochs: 10, Layers: 2, Neurons: 512, Accuracy: 0.9622, Optimizer: Adam \n",
      "Epochs: 40, Layers: 1, Neurons: 256, Accuracy: 0.962, Optimizer: SGD \n",
      "Epochs: 70, Layers: 2, Neurons: 256, Accuracy: 0.962, Optimizer: SGD \n",
      "Epochs: 70, Layers: 3, Neurons: 256, Accuracy: 0.962, Optimizer: SGD \n",
      "Epochs: 70, Layers: 2, Neurons: 512, Accuracy: 0.9616, Optimizer: SGD \n",
      "Epochs: 5, Layers: 2, Neurons: 512, Accuracy: 0.9614, Optimizer: RMSprop \n",
      "Epochs: 70, Layers: 3, Neurons: 512, Accuracy: 0.9606, Optimizer: SGD \n",
      "Epochs: 10, Layers: 4, Neurons: 512, Accuracy: 0.9602, Optimizer: RMSprop \n",
      "Epochs: 40, Layers: 1, Neurons: 256, Accuracy: 0.96, Optimizer: RMSprop \n",
      "Epochs: 5, Layers: 3, Neurons: 512, Accuracy: 0.9594, Optimizer: RMSprop \n",
      "Epochs: 10, Layers: 1, Neurons: 512, Accuracy: 0.9594, Optimizer: RMSprop \n",
      "Epochs: 10, Layers: 1, Neurons: 256, Accuracy: 0.9592, Optimizer: Adam \n",
      "Epochs: 10, Layers: 3, Neurons: 512, Accuracy: 0.959, Optimizer: RMSprop \n",
      "Epochs: 40, Layers: 4, Neurons: 256, Accuracy: 0.9588, Optimizer: RMSprop \n",
      "Epochs: 40, Layers: 3, Neurons: 512, Accuracy: 0.9586, Optimizer: RMSprop \n",
      "Epochs: 70, Layers: 4, Neurons: 256, Accuracy: 0.9586, Optimizer: SGD \n",
      "Epochs: 5, Layers: 1, Neurons: 256, Accuracy: 0.958, Optimizer: RMSprop \n",
      "Epochs: 10, Layers: 2, Neurons: 512, Accuracy: 0.9578, Optimizer: SGD \n",
      "Epochs: 40, Layers: 3, Neurons: 256, Accuracy: 0.9576, Optimizer: SGD \n",
      "Epochs: 40, Layers: 4, Neurons: 512, Accuracy: 0.9572, Optimizer: SGD \n",
      "Epochs: 70, Layers: 1, Neurons: 64, Accuracy: 0.9572, Optimizer: SGD \n",
      "Epochs: 40, Layers: 1, Neurons: 64, Accuracy: 0.9568, Optimizer: SGD \n",
      "Epochs: 40, Layers: 3, Neurons: 256, Accuracy: 0.9562, Optimizer: Adam \n",
      "Epochs: 10, Layers: 2, Neurons: 256, Accuracy: 0.9558, Optimizer: Adam \n",
      "Epochs: 5, Layers: 2, Neurons: 256, Accuracy: 0.9554, Optimizer: RMSprop \n",
      "Epochs: 10, Layers: 2, Neurons: 256, Accuracy: 0.955, Optimizer: SGD \n",
      "Epochs: 10, Layers: 2, Neurons: 512, Accuracy: 0.955, Optimizer: RMSprop \n",
      "Epochs: 10, Layers: 4, Neurons: 512, Accuracy: 0.955, Optimizer: Adam \n",
      "Epochs: 70, Layers: 4, Neurons: 512, Accuracy: 0.955, Optimizer: SGD \n",
      "Epochs: 70, Layers: 2, Neurons: 64, Accuracy: 0.9548, Optimizer: SGD \n",
      "Epochs: 40, Layers: 1, Neurons: 64, Accuracy: 0.9546, Optimizer: RMSprop \n",
      "Epochs: 40, Layers: 3, Neurons: 64, Accuracy: 0.9544, Optimizer: SGD \n",
      "Epochs: 5, Layers: 1, Neurons: 512, Accuracy: 0.9542, Optimizer: Adam \n",
      "Epochs: 10, Layers: 4, Neurons: 256, Accuracy: 0.954, Optimizer: RMSprop \n",
      "Epochs: 40, Layers: 4, Neurons: 256, Accuracy: 0.954, Optimizer: SGD \n",
      "Epochs: 70, Layers: 2, Neurons: 256, Accuracy: 0.9536, Optimizer: RMSprop \n",
      "Epochs: 5, Layers: 4, Neurons: 512, Accuracy: 0.953, Optimizer: RMSprop \n",
      "Epochs: 10, Layers: 3, Neurons: 512, Accuracy: 0.9528, Optimizer: SGD \n",
      "Epochs: 10, Layers: 3, Neurons: 512, Accuracy: 0.9526, Optimizer: Adam \n",
      "Epochs: 40, Layers: 4, Neurons: 64, Accuracy: 0.9524, Optimizer: SGD \n",
      "Epochs: 70, Layers: 1, Neurons: 64, Accuracy: 0.9524, Optimizer: Adam \n",
      "Epochs: 10, Layers: 1, Neurons: 256, Accuracy: 0.9522, Optimizer: SGD \n",
      "Epochs: 70, Layers: 1, Neurons: 64, Accuracy: 0.952, Optimizer: RMSprop \n",
      "Epochs: 10, Layers: 1, Neurons: 512, Accuracy: 0.9518, Optimizer: SGD \n",
      "Epochs: 5, Layers: 2, Neurons: 512, Accuracy: 0.9512, Optimizer: Adam \n",
      "Epochs: 5, Layers: 3, Neurons: 512, Accuracy: 0.9506, Optimizer: Adam \n",
      "Epochs: 40, Layers: 2, Neurons: 64, Accuracy: 0.9506, Optimizer: SGD \n",
      "Epochs: 5, Layers: 1, Neurons: 256, Accuracy: 0.9502, Optimizer: Adam \n",
      "Epochs: 40, Layers: 2, Neurons: 64, Accuracy: 0.9502, Optimizer: Adam \n",
      "Epochs: 70, Layers: 2, Neurons: 64, Accuracy: 0.9502, Optimizer: RMSprop \n",
      "Epochs: 10, Layers: 4, Neurons: 256, Accuracy: 0.9492, Optimizer: Adam \n",
      "Epochs: 40, Layers: 1, Neurons: 64, Accuracy: 0.949, Optimizer: Adam \n",
      "Epochs: 10, Layers: 2, Neurons: 64, Accuracy: 0.9486, Optimizer: SGD \n",
      "Epochs: 5, Layers: 2, Neurons: 256, Accuracy: 0.9484, Optimizer: Adam \n",
      "Epochs: 40, Layers: 4, Neurons: 256, Accuracy: 0.9482, Optimizer: Adam \n",
      "Epochs: 40, Layers: 4, Neurons: 64, Accuracy: 0.9478, Optimizer: Adam \n",
      "Epochs: 70, Layers: 3, Neurons: 64, Accuracy: 0.9474, Optimizer: Adam \n",
      "Epochs: 70, Layers: 4, Neurons: 64, Accuracy: 0.9474, Optimizer: Adam \n",
      "Epochs: 70, Layers: 2, Neurons: 64, Accuracy: 0.9468, Optimizer: Adam \n",
      "Epochs: 70, Layers: 4, Neurons: 64, Accuracy: 0.9468, Optimizer: SGD \n",
      "Epochs: 40, Layers: 3, Neurons: 64, Accuracy: 0.9462, Optimizer: Adam \n",
      "Epochs: 70, Layers: 3, Neurons: 64, Accuracy: 0.9462, Optimizer: SGD \n",
      "Epochs: 10, Layers: 3, Neurons: 256, Accuracy: 0.9458, Optimizer: Adam \n",
      "Epochs: 70, Layers: 1, Neurons: 32, Accuracy: 0.9454, Optimizer: SGD \n",
      "Epochs: 70, Layers: 3, Neurons: 32, Accuracy: 0.9454, Optimizer: SGD \n",
      "Epochs: 10, Layers: 4, Neurons: 512, Accuracy: 0.9452, Optimizer: SGD \n",
      "Epochs: 70, Layers: 2, Neurons: 32, Accuracy: 0.945, Optimizer: SGD \n",
      "Epochs: 10, Layers: 4, Neurons: 256, Accuracy: 0.9448, Optimizer: SGD \n",
      "Epochs: 40, Layers: 1, Neurons: 32, Accuracy: 0.9446, Optimizer: SGD \n",
      "Epochs: 40, Layers: 3, Neurons: 64, Accuracy: 0.9446, Optimizer: RMSprop \n",
      "Epochs: 5, Layers: 3, Neurons: 256, Accuracy: 0.9444, Optimizer: Adam \n",
      "Epochs: 5, Layers: 4, Neurons: 512, Accuracy: 0.9444, Optimizer: SGD \n",
      "Epochs: 10, Layers: 2, Neurons: 64, Accuracy: 0.9442, Optimizer: Adam \n",
      "Epochs: 10, Layers: 1, Neurons: 64, Accuracy: 0.944, Optimizer: SGD \n",
      "Epochs: 10, Layers: 1, Neurons: 64, Accuracy: 0.9432, Optimizer: RMSprop \n",
      "Epochs: 40, Layers: 2, Neurons: 32, Accuracy: 0.9432, Optimizer: SGD \n",
      "Epochs: 5, Layers: 3, Neurons: 256, Accuracy: 0.943, Optimizer: RMSprop \n",
      "Epochs: 5, Layers: 3, Neurons: 512, Accuracy: 0.9426, Optimizer: SGD \n",
      "Epochs: 40, Layers: 2, Neurons: 64, Accuracy: 0.9426, Optimizer: RMSprop \n",
      "Epochs: 70, Layers: 2, Neurons: 32, Accuracy: 0.942, Optimizer: Adam \n",
      "Epochs: 10, Layers: 3, Neurons: 256, Accuracy: 0.9418, Optimizer: SGD \n",
      "Epochs: 10, Layers: 3, Neurons: 64, Accuracy: 0.9416, Optimizer: SGD \n",
      "Epochs: 40, Layers: 1, Neurons: 32, Accuracy: 0.9406, Optimizer: RMSprop \n",
      "Epochs: 5, Layers: 4, Neurons: 512, Accuracy: 0.9404, Optimizer: Adam \n",
      "Epochs: 70, Layers: 1, Neurons: 32, Accuracy: 0.9404, Optimizer: Adam \n",
      "Epochs: 5, Layers: 1, Neurons: 512, Accuracy: 0.9392, Optimizer: SGD \n",
      "Epochs: 70, Layers: 4, Neurons: 64, Accuracy: 0.9392, Optimizer: RMSprop \n",
      "Epochs: 40, Layers: 3, Neurons: 32, Accuracy: 0.939, Optimizer: SGD \n",
      "Epochs: 70, Layers: 3, Neurons: 64, Accuracy: 0.939, Optimizer: RMSprop \n",
      "Epochs: 10, Layers: 3, Neurons: 64, Accuracy: 0.9386, Optimizer: RMSprop \n",
      "Epochs: 70, Layers: 2, Neurons: 32, Accuracy: 0.9386, Optimizer: RMSprop \n",
      "Epochs: 40, Layers: 4, Neurons: 64, Accuracy: 0.938, Optimizer: RMSprop \n",
      "Epochs: 5, Layers: 2, Neurons: 512, Accuracy: 0.9378, Optimizer: SGD \n",
      "Epochs: 10, Layers: 1, Neurons: 64, Accuracy: 0.9376, Optimizer: Adam \n",
      "Epochs: 5, Layers: 1, Neurons: 64, Accuracy: 0.9368, Optimizer: RMSprop \n",
      "Epochs: 5, Layers: 3, Neurons: 256, Accuracy: 0.9368, Optimizer: SGD \n",
      "Epochs: 5, Layers: 2, Neurons: 256, Accuracy: 0.9362, Optimizer: SGD \n",
      "Epochs: 5, Layers: 4, Neurons: 256, Accuracy: 0.9362, Optimizer: Adam \n",
      "Epochs: 40, Layers: 4, Neurons: 32, Accuracy: 0.9356, Optimizer: SGD \n",
      "Epochs: 10, Layers: 3, Neurons: 64, Accuracy: 0.9352, Optimizer: Adam \n",
      "Epochs: 10, Layers: 4, Neurons: 64, Accuracy: 0.935, Optimizer: SGD \n",
      "Epochs: 10, Layers: 1, Neurons: 32, Accuracy: 0.9346, Optimizer: SGD \n",
      "Epochs: 70, Layers: 3, Neurons: 32, Accuracy: 0.9344, Optimizer: Adam \n",
      "Epochs: 5, Layers: 1, Neurons: 256, Accuracy: 0.9342, Optimizer: SGD \n",
      "Epochs: 70, Layers: 1, Neurons: 32, Accuracy: 0.9336, Optimizer: RMSprop \n",
      "Epochs: 70, Layers: 4, Neurons: 32, Accuracy: 0.9336, Optimizer: SGD \n",
      "Epochs: 40, Layers: 1, Neurons: 32, Accuracy: 0.9328, Optimizer: Adam \n",
      "Epochs: 40, Layers: 2, Neurons: 32, Accuracy: 0.932, Optimizer: Adam \n",
      "Epochs: 70, Layers: 3, Neurons: 32, Accuracy: 0.9316, Optimizer: RMSprop \n",
      "Epochs: 10, Layers: 3, Neurons: 32, Accuracy: 0.9314, Optimizer: SGD \n",
      "Epochs: 40, Layers: 3, Neurons: 32, Accuracy: 0.931, Optimizer: RMSprop \n",
      "Epochs: 5, Layers: 4, Neurons: 256, Accuracy: 0.9304, Optimizer: RMSprop \n",
      "Epochs: 10, Layers: 2, Neurons: 32, Accuracy: 0.9298, Optimizer: Adam \n",
      "Epochs: 40, Layers: 4, Neurons: 32, Accuracy: 0.9296, Optimizer: RMSprop \n",
      "Epochs: 10, Layers: 1, Neurons: 32, Accuracy: 0.9294, Optimizer: RMSprop \n",
      "Epochs: 5, Layers: 2, Neurons: 64, Accuracy: 0.9286, Optimizer: RMSprop \n",
      "Epochs: 40, Layers: 3, Neurons: 32, Accuracy: 0.9286, Optimizer: Adam \n",
      "Epochs: 10, Layers: 2, Neurons: 64, Accuracy: 0.9284, Optimizer: RMSprop \n",
      "Epochs: 5, Layers: 1, Neurons: 64, Accuracy: 0.9282, Optimizer: Adam \n",
      "Epochs: 40, Layers: 4, Neurons: 32, Accuracy: 0.928, Optimizer: Adam \n",
      "Epochs: 5, Layers: 2, Neurons: 64, Accuracy: 0.9278, Optimizer: SGD \n",
      "Epochs: 5, Layers: 1, Neurons: 64, Accuracy: 0.9268, Optimizer: SGD \n",
      "Epochs: 70, Layers: 4, Neurons: 32, Accuracy: 0.9264, Optimizer: Adam \n",
      "Epochs: 5, Layers: 2, Neurons: 64, Accuracy: 0.926, Optimizer: Adam \n",
      "Epochs: 10, Layers: 4, Neurons: 64, Accuracy: 0.9252, Optimizer: RMSprop \n",
      "Epochs: 10, Layers: 1, Neurons: 32, Accuracy: 0.9244, Optimizer: Adam \n",
      "Epochs: 5, Layers: 4, Neurons: 256, Accuracy: 0.924, Optimizer: SGD \n",
      "Epochs: 40, Layers: 2, Neurons: 32, Accuracy: 0.923, Optimizer: RMSprop \n",
      "Epochs: 5, Layers: 3, Neurons: 64, Accuracy: 0.9216, Optimizer: Adam \n",
      "Epochs: 70, Layers: 4, Neurons: 32, Accuracy: 0.9208, Optimizer: RMSprop \n",
      "Epochs: 10, Layers: 4, Neurons: 32, Accuracy: 0.9204, Optimizer: Adam \n",
      "Epochs: 5, Layers: 3, Neurons: 32, Accuracy: 0.9196, Optimizer: SGD \n",
      "Epochs: 10, Layers: 2, Neurons: 32, Accuracy: 0.919, Optimizer: RMSprop \n",
      "Epochs: 10, Layers: 4, Neurons: 32, Accuracy: 0.9184, Optimizer: SGD \n",
      "Epochs: 10, Layers: 2, Neurons: 32, Accuracy: 0.9182, Optimizer: SGD \n",
      "Epochs: 5, Layers: 3, Neurons: 64, Accuracy: 0.9176, Optimizer: RMSprop \n",
      "Epochs: 5, Layers: 1, Neurons: 32, Accuracy: 0.9156, Optimizer: RMSprop \n",
      "Epochs: 10, Layers: 3, Neurons: 32, Accuracy: 0.915, Optimizer: Adam \n",
      "Epochs: 10, Layers: 4, Neurons: 64, Accuracy: 0.915, Optimizer: Adam \n",
      "Epochs: 5, Layers: 2, Neurons: 32, Accuracy: 0.9132, Optimizer: SGD \n",
      "Epochs: 5, Layers: 1, Neurons: 32, Accuracy: 0.913, Optimizer: Adam \n",
      "Epochs: 5, Layers: 1, Neurons: 32, Accuracy: 0.9128, Optimizer: SGD \n",
      "Epochs: 10, Layers: 3, Neurons: 32, Accuracy: 0.9112, Optimizer: RMSprop \n",
      "Epochs: 5, Layers: 2, Neurons: 32, Accuracy: 0.9096, Optimizer: Adam \n",
      "Epochs: 5, Layers: 3, Neurons: 32, Accuracy: 0.9048, Optimizer: Adam \n",
      "Epochs: 5, Layers: 3, Neurons: 64, Accuracy: 0.9042, Optimizer: SGD \n",
      "Epochs: 5, Layers: 4, Neurons: 32, Accuracy: 0.904, Optimizer: SGD \n",
      "Epochs: 5, Layers: 2, Neurons: 32, Accuracy: 0.9038, Optimizer: RMSprop \n",
      "Epochs: 5, Layers: 4, Neurons: 64, Accuracy: 0.9034, Optimizer: RMSprop \n",
      "Epochs: 5, Layers: 4, Neurons: 64, Accuracy: 0.9024, Optimizer: SGD \n",
      "Epochs: 5, Layers: 3, Neurons: 32, Accuracy: 0.9002, Optimizer: RMSprop \n",
      "Epochs: 10, Layers: 4, Neurons: 32, Accuracy: 0.8972, Optimizer: RMSprop \n",
      "Epochs: 5, Layers: 4, Neurons: 64, Accuracy: 0.8958, Optimizer: Adam \n",
      "Epochs: 5, Layers: 4, Neurons: 32, Accuracy: 0.8808, Optimizer: RMSprop \n",
      "Epochs: 5, Layers: 4, Neurons: 32, Accuracy: 0.877, Optimizer: Adam \n"
     ]
    }
   ],
   "source": [
    "# Test hyperparameters with different optimizers\n",
    "\n",
    "\n",
    "optimizer_configs = [\n",
    "    (\"SGD\", {\"lr\": 0.01, \"momentum\" :0.9} ),\n",
    "    (\"Adam\", {\"lr\": 0.001}),\n",
    "    (\"RMSprop\", {\"lr\": 0.001}),\n",
    "]\n",
    "\n",
    "starting_neurons_list = [32, 64, 256, 512]\n",
    "hidden_layers_list = [1, 2, 3, 4]\n",
    "epochs_list = [5, 10, 40, 70]\n",
    "\n",
    "\n",
    "best_results = {\n",
    "    \"optimizer_name\": \"\",\n",
    "    \"accuracy\": 0,\n",
    "    \"neurons\": 0,\n",
    "    \"layers\": 0,\n",
    "    \"epochs\": 0\n",
    "}\n",
    "\n",
    "# Progressbar\n",
    "max_count = len(epochs_list) * len(starting_neurons_list) * len(hidden_layers_list)\n",
    "progress = IntProgress(value=0, min=0, max=max_count)  \n",
    "display(progress) \n",
    "current_count = 0\n",
    "best_accuracy = 0\n",
    "\n",
    "test_details = []\n",
    "\n",
    "# Save models to a folder called Models in current working directory\n",
    "current_working_directory = os.getcwd()\n",
    "models_dir = os.path.join(current_working_directory, 'Models')\n",
    "if not os.path.exists(models_dir):\n",
    "    os.makedirs(models_dir)\n",
    "\n",
    "\n",
    "for epoch in epochs_list:\n",
    "    for neuron in starting_neurons_list:\n",
    "        for layers in hidden_layers_list:\n",
    "            for optimizer_name, config in optimizer_configs:\n",
    "                current_count += 1\n",
    "                progress.value = current_count\n",
    "            \n",
    "                model = MLP(hidden_layers=layers, start_neurons=neuron).to(device)\n",
    "                # print(\"***********************************************************************************************************\")\n",
    "                # print(f\"Starting training and testing with starting neurons: {neuron}, hidden layers: {layers} and {epoch} epochs.\")\n",
    "                # print(f\"This is the current model: {model}\")\n",
    "                \n",
    "                if optimizer_name == \"SGD\":\n",
    "                    optimizer = torch.optim.SGD(model.parameters(), **config)\n",
    "                elif optimizer_name == \"Adam\":\n",
    "                    optimizer = torch.optim.Adam(model.parameters(), **config)\n",
    "                elif optimizer_name == \"RMSprop\":\n",
    "                    optimizer = torch.optim.RMSprop(model.parameters(), **config)\n",
    "\n",
    "                loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "                model.train_model(train_loader=train_loader, num_epochs=epoch, loss_fn=loss_fn, optimizer=optimizer)\n",
    "\n",
    "                current_accuracy = model.test_model(test_loader=test_loader, loss_fn=loss_fn)\n",
    "\n",
    "                # Save model to a file\n",
    "                model_name = f\"model_opt{optimizer_name}_ep{epoch}_neu{neuron}_lay{layers}_acc{current_accuracy:.4f}.pth\"\n",
    "                model_path = os.path.join(models_dir, model_name)\n",
    "                torch.save(model.state_dict(), model_path)\n",
    "\n",
    "                # Store all test data\n",
    "                test_details.append({\n",
    "                    \"optimizer\": optimizer_name,\n",
    "                    \"accuracy\": current_accuracy,\n",
    "                    \"neurons\": neuron,\n",
    "                    \"layers\": layers,\n",
    "                    \"epochs\": epoch\n",
    "                })\n",
    "\n",
    "                \n",
    "                if current_accuracy > best_accuracy:\n",
    "                    best_accuracy = current_accuracy\n",
    "                    best_results['optimizer_name'] = optimizer_name\n",
    "                    best_results[\"accuracy\"] = current_accuracy\n",
    "                    best_results[\"neurons\"] = neuron\n",
    "                    best_results[\"layers\"] = layers\n",
    "                    best_results[\"epochs\"] = epoch\n",
    "                        \n",
    "test_details = sorted(test_details, key=lambda x: x['accuracy'], reverse=True)\n",
    "print(f\"The best accuracy: {best_results['accuracy']}. It was achieved with the following hyperparmeters: \\n {best_results['epochs']} epochs. \\n {best_results['neurons']} starting neurons. \\n {best_results['layers']} layers. \\n {best_results['optimizer_name']} as optimizer. \")\n",
    "for details in test_details:\n",
    "    print(f\"Epochs: {details['epochs']}, Layers: {details['layers']}, Neurons: {details['neurons']}, Accuracy: {details['accuracy']}, Optimizer: {details['optimizer']} \")                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocces single images\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def pre_process_image(image, img_shape=(-1, 28*28), norm_value=255.0):\n",
    "    # Flatten image to 1D array and normalize\n",
    "    image=image.reshape(img_shape) / norm_value\n",
    "\n",
    "    # Convert to tensor and set to gpu\n",
    "    image = torch.tensor(image, dtype=torch.float32).to(device)\n",
    "\n",
    "    return image\n",
    " \n",
    "\n",
    "# Display image\n",
    "\n",
    "def display_image(image, label):\n",
    "    plt.title( f\"Label is {label}\")\n",
    "    plt.imshow(image)\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: 4\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Demo\n",
    "\n",
    "\n",
    "# Load minst dataset\n",
    "(Xtr, Ltr), (X_test, L_test)=mnist.load_data()\n",
    "random_nr = randint(0, 1000)\n",
    "image = X_test[random_nr,:,:]\n",
    "label = L_test[random_nr]\n",
    "image_preprocessed = pre_process_image(image)\n",
    "\n",
    "# # Load the best saved model from Model folder\n",
    "# current_working_directory = os.getcwd()\n",
    "# model_name = \"model_optAdam_ep70_neu512_lay2_acc0.9724.pth\"\n",
    "# model_path = os.path.join(current_working_directory, 'Models', model_name)\n",
    "# model = MLP(hidden_layers=2, start_neurons=512)  \n",
    "# model.load_state_dict(torch.load(model_path)) \n",
    "\n",
    "# Load the best saved model \n",
    "model_name = \"model_optAdam_ep70_neu512_lay2_acc0.9724.pth\"\n",
    "model = MLP(hidden_layers=2, start_neurons=512)  \n",
    "model.load_state_dict(torch.load(model_name)) \n",
    "\n",
    "# Test random data \n",
    "model.eval()\n",
    "model.to(device)\n",
    "output=model(image_preprocessed)\n",
    "predicted_class = output.argmax(dim=1)\n",
    "print(f\"Prediction: {predicted_class.item()}\")\n",
    "display_image(image, label)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "The five best models all used Adam as optimization function. The accuracy for the best five is in the range (96,78-97.24) which means they are pretty close to eachother. The best three all used 512 neurons which is the highest I tested. Besides from that the top results was all trained with either 40 or 70 epochs and had one, two or three hidden layers. At 6th place is the first RMSprop-result with 96,76% accuracy. It used only one hidden layer, 70 epochs and 256 neurons. We have to go to the 14th best result to find the first SGD with an accuracy of 96.6%. It was trained by 70 epochs and used 512 neurons but only had one hidden layer aswell. The top four results for the SGD used either one or two hidden layers. <br>\n",
    "\n",
    "To summarize this, Adam was the best optimizer for this task but it was pretty close with the others. The most important parameter seems to be the number of neurons used and to get atleast 40 epochs of training. The number of hidden layers seems the least important but it never seems necessary to have more than three layers."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
